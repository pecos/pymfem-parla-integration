#!/bin/bash
#----------------------------------------------------
#   Script for multi-core CPU runs on 
#   TACC Frontera CLX nodes
#
#   *** Parallel Job in Small Queue***
# 
# Last revised: 16 December 2022
#
# Notes: This particular script launches a python
# script that demonstrates the integration of Parla
# with MFEM on GPU architectures.
#
#----------------------------------------------------

#SBATCH -J LF_gpu          # Job name
#SBATCH -o /scratch1/09047/was2489/LF_gpu.o%j      # Name of stdout output file (scratch user space)
#SBATCH -e /scratch1/09047/was2489/LF_gpu.e%j      # Name of stderr error file (scratch user space)
#SBATCH -p rtx-dev           # Queue (partition) name
#SBATCH -N 1               # Total # of nodes (must be 1 for serial)
#SBATCH -n 16              # Total # of cores
#SBATCH -t 00:02:00        # Run time (hh:mm:ss)
#SBATCH --mail-type=all    # Send email at begin and end of job
#SBATCH -A PHY21005        # Project/Allocation name (req'd if you have more than 1)
#SBATCH --mail-user=william.sands@austin.utexas.edu

# To use the conda environment and modules therein, we
# need to source the conda.sh file located in the user home space 
source ${HOME}/miniconda3/etc/profile.d/conda.sh

# Go to the directory of the project
cd ${WORK}/Projects/pymfem-parla-integration/code/LF_tests

# Activate the Python environment
conda activate PyMFEM_env

# Number of trials for each experiment
num_trials=1

# We use 1 block per GPU
num_gpus_list="2"

for num_gpus in $num_gpus_list  
do
    python parla_LFIntegrator_ex1_mgpu.py -ngpus $num_gpus -blocks $num_gpus -trials $num_trials
done
